{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EfficientNet\n",
    "- Paper : (2020)_EfficientNet-Rethinking Model Scaling for Convolutional Neural Networks\n",
    "- 2019 MnasNet 저자이며 Image Classification 타겟의 굉장히 성능이 좋은 Model\n",
    "- EfficientNet은 depth, width, resolution 을 잘 조합해서 compound network를 만들어 보자는 것.\n",
    "- ResNet 이후의 Two Streams\n",
    "    - Better Accuracy (더 높은 성능을 위한)\n",
    "        - 메모리, 효율성보다는 일단은 성능 위주로 (Large Model)\n",
    "        - NasNet, MNasNet, ResNet101, SENet, GPipeNet 등.. \n",
    "       \n",
    "    - Best Efficiency\n",
    "        - 성능은 어느정도 괜찮은 것 같으니.. 효율성을 취하자.\n",
    "        - MobileNet, ShuffleNet, SqueezeNets 계열 등...\n",
    "    - etc\n",
    "        - AutoML (MNasNet) 같은 느낌의 모델 방향도 나옴.\n",
    "\n",
    "- 해당 논문에서는 MNasNet과 거의 동일한 Search Space하에서 AutoML을 통해 모델을 탐색\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 모델의 성능을 올리는 가장 기본적인 접근 3가지 (Model scaling)\n",
    "- 1. Depth scailing\n",
    "    - 레이어를 깊게 쌓자\n",
    "    \n",
    "- 2. Width scailing\n",
    "    - 채널수를 늘리는 것\n",
    "    \n",
    "- 3. Image Resolution (r) scailing\n",
    "    - 이미지 해상도를 높이는 것\n",
    "    \n",
    "- 3방향에 대해 각각 하는 것이 아니라, 잘 조합된 Compound 된 모델을 뽑아보겠다.\n",
    "\n",
    "![efficientnet_modelscailing_img](img/efficientnet_modelscailing_img.png)\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Formulation\n",
    "- **실험에 대한 문제를 Formula로 정의**한 것\n",
    "- Conv NeuralNet(N)이 정의되고, inputtensor(X), spatial dimension (H,W) channel dimension(C)\n",
    "- F(Base Line)는 정의 및 고정하고, input tensor의 h,w,c를 조정하면서 실험하는 것.\n",
    "- 하지만 Design space 에 대한 부분은 남아있다.\n",
    "\n",
    "![efficientnet_formula](img/efficientnet_formula.PNG)\n",
    "\n",
    "- **Memory나 Flops를 제한(<=target memory, <=target_flops)한채 max(Accuracy(N(d,w,r)) 한 모델을 찾아보겠다**\n",
    "- d,w,r(depth, width, resolution)\n",
    "\n",
    "![efficientnet_formula2.PNG](img/efficientnet_formula2.PNG)\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Scailing Factor 변동 실험 \n",
    "- 나머지 Scaling은 고정해두고, 1가지 Scailing Factor에 대해서만 조정해가며 정확도의 변화 측정 \n",
    "![efficientnet_scaling_each](img/efficientnet_scaling_each.png)\n",
    "\n",
    "- 1. Scaling Dimensions - Width(Channel) \n",
    "    - Channel 수를 늘리게 되면 더 fine-grained features들을 뽑아주고, 학습도 더 잘된다.\n",
    "    - 하지만 wideResNet처럼 많이 늘리다보면 어느정도 올라가서 Saturated된 모습을 보인다.\n",
    "    \n",
    "- 2. Scaling Dimensions - Depth\n",
    "    - Depth를 늘리게되면 어느정도 성능이 올라가지만, 어느정도 Saturated(포화된) 된 모습을 보인다.\n",
    "    - ResNet-1000이 ResNet-101과 10배가 넘는 Layer를 가지고 있는데도 비슷한 결과를 가진다.\n",
    "\n",
    "- 3. Scaling Dimensions - Resolution\n",
    "    - Resolution은 이미지 크기에 따라 성능은 올라가지만 올리는거 대비 accuracy gain은 감소하는 형태가 나타난다. (잘 안올라간다 느낌)\n",
    "    \n",
    "#### Observation 1) 어느정도 이상 올라가면 얻는 gain 이 감소하는 모습을 보인다.\n",
    "\n",
    "\n",
    "![efficientnet_compound_scaling](img/efficientnet_compound_scaling.PNG)\n",
    "- depth(d)와 resolution(r)을 고정해두고 width만 조절하며 정확도의 변화를 측정하는 실험을 수행\n",
    "\n",
    "- 초록색 선 & 노란색 선 \n",
    "    - depth를 키우는 것 보다는 resolution을 키우는 것이 정확도가 좋음\n",
    "- 빨간색 선\n",
    "    - 1가지 or 2가지 scaling factor만 키워주는 것 보다 3가지 scaling factor를 동시에 키워주는 것이 성능이 더 좋음\n",
    "\n",
    "- Resolution 을 키우면 more layers, more channels 가 증가해주는것이 당연하다의 주장\n",
    "    - Input image가 커지면 그에 따라서 receptive field도 늘려줘야 하며\n",
    "    - 더 커진 fine-grained pattern들을 학습하기 위해 더 많은 channel이 필요한 건 reasonable 함\n",
    "    - 어떻게 이 3가지 요소들을 고려할 것인지에 대한 설명 ==> Compound Scaling\n",
    "    \n",
    "#### Obeservation 2) Resolution을 키움에 따라 layers와 Channels의 ratio를 맞춰주는 것은 중요하다\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Compound Scaling 실험\n",
    "\n",
    "- **가장 큰 핵심은 모델(F)를 고정하고 depth(d), width(w), resolution(r) 3가지를 조절하는데, 이때 고정하는 모델(F)를 좋은 모델로 선정하는 것이 가장 중요**\n",
    "\n",
    "\n",
    "- **어떻게 Compound Scaling을 테스트 해볼 것인가?**\n",
    "\n",
    "![efficientnet_compound_scaling_method](img/efficientnet_compound_scaling_method.PNG)\n",
    "\n",
    "- depth(α)는 2의 배수를 따르며, width(β), resolution(γ)는 input, output 의 2가지 측면에 따라 제곱된 값의 비율을 따름\n",
    "\n",
    "### Scail up MobileNets and ResNets\n",
    "- MobileNet v1, MobileNet v2, ResNet-50에서 그냥 단순 Scale up (width, resolution, depth) 보다 Compound Scale이 더 좋은 것을 확인\n",
    "\n",
    "![efficientnet_scailup_mobilenet_resnet](img/efficientnet_scailup_mobilenet_resnet.PNG)\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## EfficientNet Architecture\n",
    "- MBConv는 Inverted BottleNeck구조 (Mobilenet v2 구조) / L은 반복횟수 / \n",
    "\n",
    "![efficientnet_model_b0](img/efficientnet_model_b0.PNG)\n",
    "\n",
    "- Step1) 파이(Φ)를 1로 지정 = 네트워크가 2배로 키워지는것 = 어떤 값이 가장 좋은 값인가 (GridSearch를 이용한 탐색) => α=1.2, β=1.1, γ=1.15\n",
    "\n",
    "- Step2) Optimal 한 αβγ를 찾은 상태에서 파이(Φ)를 키워 해당 모델을 EfficientNet-B1 to B7 까지 만듬\n",
    "\n",
    "\n",
    "### ImageNet Results for EfficientNet\n",
    "- ConvNet과 AutoML을 통해 찾은 ConvNet들과 비교를 한 결과\n",
    "- **비슷한 Accuracy를 가졌던 ImageNet모델들에 대해 EfficientNet을 진행한 것**\n",
    "- **얼마나 Parameter를 효율적으로 쓰지 못했는지 보여줌**\n",
    "- **즉, ConvNet들에 비해 비슷한 정확도를 보이면서 parameter수와 FLOPS 수를 굉장히 많이 절약할 수 있음**\n",
    "\n",
    "![efficientnet_architecture](img/efficientnet_architecture.PNG)\n",
    "\n",
    "![efficientnet_vs_imagenet_result](img/efficientnet_vs_imagenet_result.PNG)\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Time Latency\n",
    "\n",
    "- 추론 시간 비교  / 파라미터차이가 몇십배가 난다고 해서 속도차이도 몇십배가 나는건 아님\n",
    "\n",
    "![efficientnet_inference_latency](img/efficientnet_inference_latency.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Transfer Learning Results for EfficientNets\n",
    "- **유명  Datasets에 대해 Sota 모델에 대한 Transfer Learning 으로 비교했을 때 총 8개중 5개가 능가하는 결과를 보임** \n",
    "\n",
    "- 그래프는 더 왼쪽 위에 있어서 보다 효율적인 것\n",
    "\n",
    "![efficientnet_transferlearning_result](img/efficientnet_transferlearning_result.PNG)\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficient Net만 가지고 순수 Scaling 비교\n",
    "- 혹시 해당 Base 모델들에 대해서만 그러한 결과를 보인 것이 아닌지에 대한 반박\n",
    "\n",
    "![efficientnet_scaliup](img/efficientnet_scaliup.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EfficientNet - Class Activation Map\n",
    "\n",
    "![efficientnet_activation_map](img/efficientnet_activation_map.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## EfficientNet Result\n",
    "- Class Activation Map(CAM)\n",
    "\n",
    "![efficientnet_result](img/efficientnet_result.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 참고문헌\n",
    "- https://www.youtube.com/watch?v=Vhz0quyvR7I // PR169- EfficientNet\n",
    "- https://hoya012.github.io/blog/EfficientNet-review/?fbclid=IwAR1gwSEAF8nqcWPhXzb39SYmJ1kQ6X6rFoZWjzmc05-dBHuBKeltagkUDZI // hoya Efficient Net 정리글"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
