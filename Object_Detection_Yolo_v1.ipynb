{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1 Stage Detector vs 2 Stage Detector \n",
    "\n",
    "![stage_1_2_detector](img/stage_1_2_detector.png)\n",
    "- 해당그림에서 선 위가 2-Stage Detector, 선 아래가 1-Stage Detector\n",
    "- **1-stage Detector는 비교적 빠르지만 2-stage detector는 비교적 느리지만 정확도가 높다**\n",
    "\n",
    "- Stage 2 Detector -> Regional Proposal과 Classification이 **순차적으로** 이루어지는 것.\n",
    "    - R-CNN 계열 (R-CNN, Fast R-CNN, Faster R-CNN, Mask R-CNN)\n",
    "![stage_2](img/stage_2.png)\n",
    "\n",
    "- Stage 1 Detector -> Regional proposal과 Classification이 **동시에** 이루어지는 것\n",
    "    - YOLO 계열, SSD 계열 ( SSD, DSSD, DSOD, RetinaNet, RefineDet, M2Det)\n",
    "![stage_1](img/stage_1.png)\n",
    "\n",
    "![model](img/model.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLO v1\n",
    "- YOLO Paper : You Only Look Once:Unified, Real-Time Object Detection\n",
    "- http://pjreddie.com/yolo/\n",
    "- 이미지 전체에 대한 하나의 신경망(end to end)\n",
    "- **단 한 번의 계산만으로 Bounding box와 클래스 확률(class probability)을 예측**\n",
    "- 1-stage Detector로 객체검출을 하나의 회귀문제(Single regression problem)로 봄\n",
    "\n",
    "![YOLO](img/YOLO.PNG)\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ■ YOLO의 장점과 단점\n",
    "- 장점\n",
    "    - (1) 단일 신경망 구조로 구성이 단순하고 매우 빠름\n",
    "        - 모델의 학습과 최적화에 큰 이점을 줌. \n",
    "        - R-CNN과 같은 복잡한 모델일수록 Stage 별 개별학습이라 최적화가 힘듬\n",
    "    - **(2) YOLO는 주변 정보까지 학습하여 이미지 전체를 처리하기 때문에 Background error가 작다.**\n",
    "        - 이미지 전체를 본다 -> 즉 배경정보도 처리하고, R-CNN은 배경에 반점이나 노이즈가 있으면 그것을 물체로 인식\n",
    "    - **(3) 물체의 일반적인 부분을 학습하기 떄문에 보다 일반화에 강건**\n",
    "        - Object의 일반적인 부분 학습\n",
    "    - (4) YOLO는 훈련과 테스트 단계에서 이미지 전체를 보게 됨.\n",
    "        - 클래스의 모양 정보뿐 아니라 주변 정보도 학습하여 처리\n",
    "\n",
    "- 단점\n",
    "    - 작은 Obejct에 대해 상대적으로 낮은 정확도(MAP)를 보인다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## YOLO Netowork\n",
    "- (1) YOLO는 입력 이미지를 S x S grid 로 나눈다.\n",
    "- (2) 각각의 grid cell은 B개의 Bounding box를 가지며 각각은 그 Bounding Box에 대한 confidence score를 예측\n",
    "- (3) Grid Cell 내에서의 Bounding Box 계산\n",
    "    - 즉 0~1의 값을 가짐\n",
    "        - ex1) 셀 중앙시 (x, y) = (0.5, 0.5)\n",
    "        - ex2) 이미지 전체의 너비와 높이 (w, h)\n",
    "- (4) 각각의 Grid Cell은 클래스별 Conditional class probabilities(C)를 예측\n",
    "    - C = Pr(Class | Object) = 해당 클래스에 대한 조건부 확률 예측 (0~1)\n",
    "- (5) 실제 Bounding Box와 예측 Bounding Box를 통한 Confidence 추출\n",
    "     \n",
    "![yolo_image](img/yolo_image.png)\n",
    "\n",
    "## IOU (Intersection over union)\n",
    "- **객체의 실제 bounding box와 예측 bounding box의 합집합 면적 대비 교집합 면적의 비율**\n",
    "- (실제 bounding box와 예측 bounding box의 교집합) / (실제 bounding box와 예측 bounding box의 합집합)\n",
    "- Pr(Object)=0 : 그리드 셀에 아무 객체가 없음. Confidence score\n",
    "- Pr(Object)=1 : 그리드 셀에 어떤 객체가 확실히 있음\n",
    "- **Confidence score와 IOU가 같다면 가장 이상적인 Score**\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Network Design\n",
    "- Input image를 7x7 grid로 나누고 진행한 것을 기존 Backbone layer와 연결해서 쓰는 것\n",
    "- 그림은 즉 아래것이 계산 후 위로 옴\n",
    "\n",
    "![yolo_network2](img/yolo_network2.jpg)\n",
    "\n",
    "- GoogleNet 모델 기반 (No 인셉션 모듈, 1x1 reduction layer 만 사용)\n",
    "- 24 Conv layers & 2 Fully Connected layer \n",
    "- 마지막계층은 선형활성화함수, 나머지 계층은 leaky ReLU\n",
    "- 구조상 문제 해결을 위한 3가지 개선안\n",
    "    - localization loss와 classification loss중 localiztaion loss의 가중치 증가\n",
    "    - 객체가 없는 그리드 셀의 confidence loss보다 객체가 존재하는 그리드 셀의 confidence loss의 가중치 증가\n",
    "    - 과적합 막기 위한 dropout, data augmentatition 적용\n",
    "- **98개의 class specific confidence score에 대해 각 20개의 클래스를 기준으로 non-maximum suppression을 진행**\n",
    "\n",
    "- 최종 Output \n",
    "    - **클래스 확률(Class probabilities)**\n",
    "        - **classification loss**\n",
    "    - **Bounding box 위치정보(coordinates)**\n",
    "        - **localization loss**\n",
    "        - **너비 (width), 높이(height), bounding box 중심좌표(x,y)**\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## YOLO Loss Function - Training\n",
    "- **바운딩 박스를 잘 그렸는지 평가하는 Localization Error와 박스 안의 물체를 잘 분류했는지 평가하는 Classification Error의 패널티를 다르게 평가**\n",
    "\n",
    "- Localization Error : 바운딩 박스를 잘 그렸나?\n",
    "    - Bounding box coordinate regression & Bounding box score prediction\n",
    "    - SSE (Sum-squared-error) : Regression 문제처럼 생각. Regression loss 사용. \n",
    "    - 최종 아웃풋의 SSE(sum-squared error)를 최적화(optimize) 하는 것이 목표.\n",
    "    - x, y, w, h(w와 h는 root 씌워서 값을 낮춤-normalize)\n",
    "- Classification Error : 박스 안에 물체를 잘 분류했는가?\n",
    "    - Bounding box는 상관없고, 각 grid셀마다 클래스를 분류.\n",
    "\n",
    "- Object가 있는 경우, 없는 경우보다 큰 loss를 줌 \n",
    "    - 감마 coord는 물체인경우로 5로 높은 패널티를 부여, 감마 noobj는 배경인 경우로 0.5의 비교적 낮은 패널티 부여\n",
    "    \n",
    "- 많은 바운딩 박스중에 IOU 수치가 가장 높게 생성된 바운딩 박스만 학습에 참여\n",
    "    - 이는 바운딩 박스를 잘 만드는 셀은 더욱 학습을 잘하도록 높은 Confidence Score를 주고\n",
    "    - 나머지 셀은 바운딩 박스를 잘 만들지 못하더라도 나중에 Non-max suppression을 통해 최적화 하기 위함이다\n",
    "\n",
    "### 수식 설명\n",
    "- Localization - 첫줄이 좌표 위치에 대한 loss / 두번째줄은 box 크기에 대한 loss\n",
    "- 감마 coord, noobj는 5, 0.5로 Localization 부분 (첫째노랑, 둘쨰줄-분홍) 부분에 큰 패널티를 주기 위해서. (보면 마지막줄 Class score prediction)에는 감마가 붙어있지 않다.\n",
    "- 큰 물체에 대해서는 루트를 씌워서 영향을 조금 줄임\n",
    "\n",
    "![yolo_loss_function](img/yolo_loss_function.jpg)\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLO Inference\n",
    "- 위 Loss Function을 이용해서 네트워크를 학습 후 예측을 하면 각 셀마다 여러 장의 바운딩 박스가 생김 (한 이미지당 98개의 Bounding Box)\n",
    "\n",
    "- **물체의 중심을 중심으로 그려진 바운딩 박스는 Confidence Score가 더 높게 나오고, 물체의 중심으로 부터 먼 셀이 만드는 바운딩 박스는 Score가 작게 나옴**\n",
    "- 그리고 **최종적으로 여러 개의 바운딩 박스를 합치는 Non-max suppression 과정을 거쳐 이미지의 객체를 탐지**\n",
    "- 최종 아웃풋은 한 피쳐맵\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## YOLO NMS\n",
    "- **Grid 디자인 시 하나의 객체를 여러 그리드 셀이 동시에 검출하는 경우 발생가능**\n",
    "    - ex) 객체의 크기가 너무 큰 경우\n",
    "    - ex2) 객체가 그리드 셀 경계에 인접해 있는 경우\n",
    "- 해당 경우를 다중검출(multiple detections) 문제라고 함\n",
    "- **비 최대 억제 (non-maximal suppression) 를 통한 개선** \n",
    "    - Non Maximum Suppression 으로 Leave only one Bounding Box\n",
    "    - 바운딩 박스가 많을 때 최종 하나만 남기게 되는 것\n",
    "    - (1) 동일한 클래스에 대해 confidence 정렬\n",
    "    - (2) 가장 confidence 가 높은 bounding box를 기준으로 IOU가 Threshold를 넘는 Bounding box 제거\n",
    "    - (3) 제거되지 않는 box들 중 (2)에서 선정된 박스 제외후 (1)~(2) 과정 반복\n",
    "    - (4) 위 과정 반복시 중복된 box들은 모두 제거되고 거리가 떨어진 유의미한 Box들만 살아남음\n",
    "\n",
    "![yolo_nms](img/yolo_nms.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Limitation of YOLO\n",
    "- **각각의 grid cell이 하나의 클래스만을 예측하므로 작은 Object 여러개가 겹칠시 예측 불가**\n",
    "- 훈련단계에서 학습하지 못한 종횡비를 테스트 단계에서 마주치면 고전\n",
    "- 큰 Bounding box와 작은 Bounding box의 loss에 동일 가중치를 둔다는 점\n",
    "    - 크기가 큰 Bounding box는 위치가 조금 달라져도 성능에 영향을 주기 힘듬\n",
    "    - 크기가 작은 Bounding box는 위치가 조금 달라져도 성능에 큰 영향 줄 수 있음\n",
    "- 몇단계의 Layer를 거쳐 나온 Feature map을 대상으로 Bounding Box를 예측하여 Localization이 다소 부정확해 질 수 있음\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 참고문헌\n",
    "- https://transformer.tistory.com/entry/You-Only-Look-Once-Unified-Real-Time-Object-Detection-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0\n",
    "\n",
    "- https://curt-park.github.io/2017-03-26/yolo/\n",
    "- https://bkshin.tistory.com/entry/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-YOLOYou-Only-Look-Once"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
