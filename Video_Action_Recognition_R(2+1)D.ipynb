{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9ed6d3c",
   "metadata": {},
   "source": [
    "## Video Action Recognition R(2+1)D\n",
    "- Paper : (2017) https://arxiv.org/abs/1711.11248\n",
    "    - A Closer Look at Spatiotemporal Convolutions for Action Recognition\n",
    "    \n",
    "- 3D conv를 공간정보와 시간정보로 나눠 conv하는 구조를 처음 제안\n",
    "    - 3D convolutional filters (N*t*d*d)를 (N*1*d*d) 와 (M*t*1*1)로 분해\n",
    "    - 기존 3D conv와 파라미터 동일 / Non-linearities 증가 / Optimization 도움\n",
    "- 다양한 spatiotemporal conv 방법들을 비교 \n",
    "    - R2D F-R2D, MCx, rMCx, R3D, R(2+1)D\n",
    "- RGB, Flow 두 부분에 대해 R(2+1)D 모델에 각각 학습 및 Prediction Averaging\n",
    "- Sports-1m, kenetics, ucf101, hmdb51에서 state-of-the-art를 달성\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b4cc65",
   "metadata": {},
   "source": [
    "## 1. Abstract & Introduction\n",
    "- 기존은 2d conv나 3d conv 나 비슷한 결과를 보이며 3d-conv에 대한 연구가 더딤\n",
    "\n",
    "- 시간적 정보는 action recognition의 정확한 행동인식에 필수적이지 않다, 이미 정적 frame들이 action class에 대해 정보를 가지고 있기 때문\n",
    "    - temporal reasoning is not essential for accurate action recognition, \n",
    "    - because of the strong action class information already contained in the static frames of a sequence.\n",
    "\n",
    "- 3D Conv Filters -> separate spatial and temporal components\n",
    "    - 3D conv를 두개의 개별적인 2D 공간 conv와 1D 시간 conv로 분해 하는 것\n",
    "    - 필터를 별도의 공간 및 시간 구성 요소로 분해시 정확도가 크게 향상\n",
    "    - a decomposition of 3D convolution into **a 2D spatial convolution followed by 1D temporal convolution**, which we name (2+1)D convolution\n",
    "\n",
    "<hr>\n",
    "\n",
    "### 1-1 Advantages\n",
    "- 장점\n",
    "    - (1) 파라메터가 증가하지 않음(2d 사용), relu를 두번 사용할 수 있기때문에 비선형성을 증가 (nonlinear rectification)\n",
    "    - (2) Facilitates the Optimization (2+1)D 블록이 최적화 하기 더 쉬움 \n",
    "        - the (2+1)D blocks (with factorized spatial and temporal components) are easier to optimize\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e39cf2",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Convolutional residual blocks for video\n",
    "\n",
    "![r2+1d_model](img/r2+1d_model.PNG)\n",
    "\n",
    "### 2-1. R2D\n",
    "- 2D convolutions (2D CNN 방법, 3L x H x W, 1프레임당 3채널씩 할당)\n",
    "- 프레임을 채널로 쌓아서 사용하는 방법\n",
    "\n",
    "### 2-2. f-R2D\n",
    "- 2D convolutions over frames, f-R2D는 2D CNN방법\n",
    "- 전체의 프레임을 개별적으로 2D CNN 하는 방법\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46e174c",
   "metadata": {},
   "source": [
    "\n",
    "### 2-3. MCx and rMCx: mixed 3D-2D convolutions\n",
    "- 3D convolution이 초기 계층에서 특히 유용할 수 있는 반면, 후반 계층에서는 필수적이지 않다의 생각\n",
    "- (MCx) 학습 초기에는 3D Conv를사용 + 후반에는 2D conv 사용\n",
    "- (rMCx) 학습 초기에는 2D Conv를 사용 + 후반에는 3D conv 사용\n",
    "\n",
    "### 2-4. R3D: 3D convolutions\n",
    "- R3D는 가장 일반적인 3D conv 방법 (3 x L x H X W)\n",
    "- 프레임을 시간축을 하나 추가하여 stack하고 3D conv를 하는것 입니다.\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cd3180",
   "metadata": {},
   "source": [
    "### 2-5. R(2+1)D: (2+1)D convolutions\n",
    "- (좌측) R(2+1)D와 R3D간의 비교 \n",
    "    - R(2+1)D eases the optimization as depth is increased\n",
    "    - 시공간 분해가 유효함을 증명\n",
    "\n",
    "- (우측) **Full 3D conv를 2D의 공간 conv와 1D 시간 conv로 분해**\n",
    "    - **3D convolutional filters($N*t*d*d$)를 $N*1*d*d$ 와 $M*t*1*1$로 분해** \n",
    "    - 기존의 3D conv와 동일 및 Non-linearities 증가 시키고 Optimization에 도움을 줌\n",
    "    - 하이퍼 파라미터 $M_i$ 는 공간과 공간 사이에 신호가 투영되는 중간 부분 공간의 차원을 결정\n",
    "\n",
    "![r2+1d_factorization_r3d](img/r2+1d_factorization_r3d.png)\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2c421d",
   "metadata": {},
   "source": [
    "## 3. Experiments\n",
    "- 오버피팅이 나타나지 않기 위하여 충분히 큰 데이터셋인 Sport 1M, Kinetics을 사용 학습\n",
    "- 작은 데이터셋에서 잘 작동하나 궁금하기 때문에, UCF101,HMDB51에 transfer learning을 적용\n",
    "- 실험의 good performance and simplicity을 위해 18-Layer와 34-Layer 사용\n",
    "\n",
    "### 3.1 Model Architecture\n",
    "\n",
    "![r2+1d_model_architecture](img/r2+1d_model_architecture.PNG)\n",
    "\n",
    "- R3D 18-Layer, 34-Layer 먼저 생성\n",
    "- 112 × 112 크기의 L RGB 프레임으로 구성된 클립을 입력으로 사용\n",
    "- conv1에서 1×2×2의 Conv stride에 의해 시공간 다운 샘플링\n",
    "- conv3 1, conv4 1, conv5 1에서 2×2×2의 conv stride 3개 시공간 다운 샘플링 (=stride 이용 함)\n",
    "\n",
    "- R2D, MCx, rMCx, R(2+1)D\n",
    "    - 3D 컨볼 루션을 각각 2D Conv, Mixed Conv, Reversed Mixed Conv, (2+1)D Conv로 대체\n",
    "- 시공간 GAP, Softmax (FC Layer)\n",
    "\n",
    "### 3.2 Optical Flow - Two Stream\n",
    "- RGB 및 Optical flow Input 모두에 대해 R(2+1)D 아키텍처 훈련\n",
    "    - We train our R(2+1)D architecture on both RGB and optical flow inputs\n",
    "- fuse the prediction scores by averaging\n",
    "- Farneback의 방법 사용 Optical flow을 계산\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85525d0",
   "metadata": {},
   "source": [
    "## 4. Training\n",
    "- Kinetics 이용 학습\n",
    "- Video frames scaled 128×171 -> randomly cropping 112×112.\n",
    "- We randomly sample L consecutive frames with temporal jittering\n",
    "- models are trained on 8-frame clips (L = 8) and 16-frame clips (L = 16)\n",
    "- Batch normalization is applied to all convolutional layers\n",
    "- initial learning rate is set to 0.01 and divided by 10 every 10 epochs\n",
    "- caffe2 , SGD, GPU Cluster\n",
    "\n",
    "![r2+1d_result](img/r2+1d_result.PNG)\n",
    "- R2D부터 R(2+1)D까지 모든 net을 clip단위와 video단위로 평가\n",
    "- Kinetics valid 데이터셋 accuracy 결과\n",
    "- R(2+1)D 은 R3D와 Params 차이는 비슷,  Accuracy는 가장 높음\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c536ab",
   "metadata": {},
   "source": [
    "## 5. Results\n",
    "![r2+1d_result1](img/r2+1d_result1.png)\n",
    "\n",
    "![r2+1d_result2](img/r2+1d_result2.png)\n",
    "\n",
    "![r2+1d_result3](img/r2+1d_result3.png)\n",
    "\n",
    "- 대부분의 지표가 높은 성능을 보임\n",
    "- 단 I3D Two-Stream보다 낮은 이유를 보이는 건 I3D는 TV-L1, R(2+1)D 는 Farnebacks's optical flow라서 그렇다고 함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8adb61b",
   "metadata": {},
   "source": [
    "## 참고문헌\n",
    "- https://blog.airlab.re.kr/2019/09/R(2+1)D"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
