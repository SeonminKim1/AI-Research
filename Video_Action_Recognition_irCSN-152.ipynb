{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Action Recognition irCSN-152\n",
    "- Paper : (2019) irCSN152 - Large-scale weakly-supervised pre-training for video action recognition\n",
    "\n",
    "- Video Action Recognition 간 해당 Model을 사전 교육하기 위해 많은 양의 웹 비디오를 사용하는 것의 방향성에 대한 심층 연구\n",
    "- 동영상에는 noise in Label space, temporal localization 이 있음에도, IG-Kinetics- pre-trained된 모델을 통해 강한 모델을 만듬\n",
    "- Effect of scale 많은 데이터를 학습할 수록 좋음\n",
    "    - 우리의 모델은 확장성이 좋음\n",
    "- Design of the pre-training label space\n",
    "    - Label 조합을 잘 구성하는 것이 중요\n",
    "- Effect of temporal noise에 대해 실험 및 결과를 도출\n",
    "    - 같은 video 갯수에서는 long 데이터 일수록 좋고\n",
    "    - 총 길이가 같은 경우는 video가 다양할 수록 좋다.\n",
    "    \n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Abstract & Introduction\n",
    "- Fully-supervised video 데이터셋은 수십만 개의 비디오와 1000 개 미만의 도메인 별 레이블로 구성되는데, 이는 Advanced Video Architecture로의 발전을 방해 할 수 있다.\n",
    "- Large-scale Dataset에 대한 Pre-training and Fine-tuning에 대한 성능은 이미 오래 검증되옴.\n",
    "\n",
    "### 1.1 Weakly-Supervised Learning \n",
    "- Weakly-Supervised Learning을 위한 사전 훈련 비디오 데이터 세트의 구성에 대한 세 가지 질문을 검토\n",
    "- 1. Transfer-learning에 가장 도움이 될 동사-객체 사전 훈련 레이블 공간을 어떻게 구성해야 할지\n",
    "    - ex) 물고기 잡기 (관측 및 분류 o) / 베이글 잡기 (관측 및 분류 x)\n",
    "    - (동사, 명사) 레이블의 공동 분포에서 관찰 된 부분에 대해 사전 학습해야합\n",
    "- 2. 모델이 좋은 이미지 특성, 시공간 특성에 대한 사전 훈련이 충분한지? (전이학습 할만한 지)\n",
    "- 3. 비디오 수 또는 분을 고려할 때 최상의 성능을 위해 비디오 클립은 어떻게 설정할지 \n",
    "    - 긴 비디오에는 더 많은 프레임이 포함되지만 짧은 비디오에는 더 관련성이 높은 프레임이 포함되어있을 수 있음. \n",
    "    \n",
    "    \n",
    "### 1.2 Main Keyword Syntex\n",
    "- Large-scale weak-supervision is extremely beneficial\n",
    "    - weak-supervision이 일반 Web 데이터셋들이 가지고 있는, temporal noise와, label noise에 대한 문제를 넘어설 수 있음.\n",
    "- Impact of data volume and model capacity\n",
    "    - 데이터 크기, 데이터 샘플링 전략, 모델 용량 등의 효과비교\n",
    "- What is a suitable pre-training video label sapce?\n",
    "- Short vs Long videos for pre-training?\n",
    "    - 긴 동영상을 선택하는 것이 더 유리\n",
    "- Do we need pre-training on video data?\n",
    "- State of the art results\n",
    "    - Kinetics에서 최고 정확도 81.3 %를 달성 (이전 대비 +3.6%)\n",
    "    - R(2+1)D 를 통해 학습\n",
    "    - Something-something 51.6% (이전 대비 +2.1%)\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset\n",
    "- 소셜 미디어 웹 사이트(Instagram) 에서 6천 5백만 개의 사용자 생성 공개 동영상활용 및 1 개의 관련 해시 태그를 Label 로 사용\n",
    "- Fully-supervised Video Dataset(HMDB, Kinetics, UCF.. )와 달리, Labeling 비용발생 x, 수십억 규모로 확장 가능\n",
    "\n",
    "- 구성 방법\n",
    "    - 4개의 seed action label sets 설정, label과 관련된 비디오 수집\n",
    "        - 'IG-Seed데이터셋이름-비디오크기' 로 표현\n",
    "    - 일반적으로 비디오에서의 샘플링 진행시 제곱근 샘플링 방식이 제일 좋음\n",
    "    - **데이터의 동영상 길이는 1~60 Frame임. 그러나 Label은 하나이므로 temporal noise가 발생할 수 있으며, 이런 Noise를 포함한 결과**\n",
    "    - 데이터 셋트 및 사용된 hashtag등을 유저에게 공개 불가(Instagram), 및 연구자들이 재현 불가\n",
    "    \n",
    "![ircsn-dataset](img/ircsn-dataset.png)\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pre-training Set-up\n",
    "- Models: R(2+1)D-d\n",
    "    - d denotes model depth = {18, 34, 101, 152}\n",
    "- Loss Function\n",
    "    - use softmax activations with cross-entropy loss.\n",
    "    \n",
    "- Video frames\n",
    "    - 128 × 171 to cropping a random patch of size 112 × 112 from frame.\n",
    "    - Video clips of either 8 or 32 frames\n",
    "    - temporal jittering\n",
    "    \n",
    "- Training\n",
    "    - SGD / BN (All Conv layer) \n",
    "    - Learning rate initalize 0.192 and 총 13번 감소\n",
    "    - Traininig by 128 GPUs across 16 machines\n",
    "    - Caffe2\n",
    "    - All pretraining experiments process 490M videos in total\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Experiments\n",
    "\n",
    "- Fine tunning 방법과, FC 방법 중 사용\n",
    "- 3가지 방향에 대해 실험\n",
    "    - (a) effect of scale (모델 용량, pre-training 데이터 크기)\n",
    "    - (b) design of the pre-training label space \n",
    "        - 동사, 명사 조합 레이블 집합을 체계적으로 구성 하는 법\n",
    "        - ex) IG-Kinetics-19M, IG-Verb-19M, IG-Noun-19M, and IG-Verb+Noun-19M\n",
    "    - (c) temporal properties of videos (비디오 시간적 속성)\n",
    "\n",
    "![ircsn-experiments](img/ircsn-experiments.PNG)\n",
    "\n",
    "### 4.1.(a). Effect of scale\n",
    "- IG-Kinetics- {500K, 1M, 5M, 10M, 19M, 65M} 데이터셋 구성\n",
    "- 학습 데이터 크기에 따라 성능이 로그 선형 적으로 향상\n",
    "- 더 많은 사전 학습 데이터 할 수록 더 좋음\n",
    "\n",
    "![ircsn-experiments2](img/ircsn-experiments2.PNG)\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.(b). Design of the pre-training label space\n",
    "- IG-Kinetics-19M, IG-Verb-19M, IG-Noun-19M, IG-Verb + Noun-19M을 사전 훈련 데이터 세트로 사용\n",
    "![ircsn-experiments3](img/ircsn-experiments3.PNG)\n",
    "\n",
    "- 각 (a), (b), (c), (d)는 어떤 것으로 사전학습 됬는지 의미\n",
    "- IG- 명사에 대한 사전 훈련은 EPIC-Kitchens의 명사 예측 작업에 가장 큰 도움이되는 반면 IG-Verb는 동사 예측 작업에 크게 도움이됩\n",
    "- Found overlap of 62% between IG-Verb - verb labels (Epic Kitchens)\n",
    "- Found overlap of 42% between IG-Noun - noun labels (Epic Kitchens)\n",
    "- **즉 IG-(Verb+Noun) is the most well-suited pre-training label space for EPICKitchens-actions task.** \n",
    "- 결과로도 IG-(Verb+Noun)이 해당 Task에 좋은 결과를 냄\n",
    "- **사전 학습 데이터 선택할 시 최대한 label space가 많이 겹치는 사전 학습 레이블을 선택하는 것이 좋은 학습 방법이며 좋은 라벨 공간을 구성하는 것이 엔지니어의 주요 과제임을 강조**\n",
    "\n",
    "![ircsn-experiments4](img/ircsn-experiments4.PNG)\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.(c) Effect of temporal noise\n",
    "- IG-Kinetics에서 3개의 Dataset 구성\n",
    "    - (i) short-N : 1~5 초 길이의 N 개의 동영상\n",
    "    - (ii) long-N : 55-60 초 길이의 N 개의 동영상\n",
    "    - (iii) long-center-N : long-N 비디오의 중앙 부분에서 구성된 N 개의 비디오 (약 4초 길이)\n",
    "    \n",
    "#### Case 1) Fixed Number of videos : 비디오 갯수\n",
    "- short-5M, long-center-5M은 비디오 당 시간이 비슷, But. long-center-5M이 Temporal noise 가 더 큼\n",
    "- short-5M과 long-5M 사이에서 short-5M은 시간적 위치가 더 좋고 long-5M은 content diversity이 더 크며, **long-5M이 3.2% 더 나은 성능을 보임**\n",
    "    - 즉 긴 동영상의 다양한 콘텐츠가 Temporal noise 영향을 가릴 수 있음\n",
    "- longer videos may benefit transfer learning than short videos.\n",
    "\n",
    "#### Case 2) Fixed video time : 시간 동일하게\n",
    "- short-5M vs long-center-5M and long-500K datasets (total video hours 동일하게)\n",
    "- **short-5M이 long-500K을 능가하며, 다양성, temporal localization에 대해 short video가 더 이점을 가져감 (적은 long videos 보다)\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 5. Pretraining by clips and images\n",
    "\n",
    "![ircsn-pretrain1](img/ircsn-pretrain1.PNG)\n",
    "\n",
    "- 18 개의 레이어 2D 깊은 잔차 모델 (R2D)을 사전 학습 및 해당 모델을 R3D8로 확장\n",
    "- Pre-training R3D에서 Video Clips 을 직접 학습한 모델은 71.7% accuracy로 가장 높았음.\n",
    "- 대규모 Pre-training이 Temporal structure를 효과적으로 모델링 한다는 것을 의미\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Results\n",
    "- 10 개의 클립을 균일하게 샘플링하는 대신 SC-Sampler를 사용 및 테스트에서 10 개의 두드러진 클립을 샘플링\n",
    "- Kinetics 에서의 결과\n",
    "\n",
    "![ircsn-result1](img/ircsn-result1.PNG)\n",
    "- \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
