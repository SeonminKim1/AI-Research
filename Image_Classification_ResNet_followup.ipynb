{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet 후속논문\n",
    "- paper : Identity Mappings in Deep Residual Networks\n",
    "- **ResNet에 적용된 Identity Mapping의 성공을 수식을 통해 분석하고, 최적의 Residual Network 구조는 무엇인지에 대한 실험을 진행**\n",
    "- ResNet이 왜 좋은 성능이 나오나 검증하는 논문의 성격\n",
    "- 논문에서 기존의 논문이 제시한 구조 외의 이런저런 변형을 가했을 때 성능이 어떻게 변하는지도 검증\n",
    "\n",
    "## Identify Mapping 이란\n",
    "- Identify Mapping이란 네트워크의 shortcut으로 구현\n",
    "- 이를 바탕으로 Feed Forwarding과 Backpropagation시에 직접적인 전파를 가능\n",
    "- 이러한 ＇Clean＇한 정보의 통로는 최적화에 도움이 됨\n",
    "\n",
    "- **즉 일반적인 CNN의 경우에는 최종 결과값이 수많은 행렬들의 곱셈으로 표현되는데, ResNet은 이 Clean한 루트인 Shortcut 덕분에 결과값을 간단히 Residual Unit들의 덧셈으로 표현할 수 있게 되어, Backpropagation시에도 Vanishing 문제가 일어나지 않는다.**\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 요약\n",
    "- Shortcut구조에서 여러가지 기법을 적용한다음 (+)를 해주면 어떻게 될까에 대한 것\n",
    "    - 훼손 안하는 것이 제일 좋음\n",
    "- Conv간 순서 재설정 (BN-Relu-Conv) 순서가 가장 좋은 결과를 보임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 다양한 Identify Mapping 실험 - shortcut\n",
    "![identify_mapping](img/identify_mapping.jpg)\n",
    "\n",
    "- (b)Constant Scaling : F에는 적용하지 않은 경우, 1−λ=0.5  만큼 적용한 경우로 구분 가능\n",
    "\n",
    "- (c)Gating의 경우에는 g(x)=σ(Wgx+bg)를 적용한 것, 여기서 g(x)g(x)는 1x1 Conv와 Sigmoid를 의미함.\n",
    "\n",
    "- (d)Exclusive gating의 경우에는 F는 g(x)만큼 곱해주고(element-wise), shorcut path에는 1−g(x)를 곱해준 것 shorcut-only gating의 경우에는 shorcut path만 1−g(x)를 적용.\n",
    "\n",
    "- (e)shortcut에 1x1Conv를 적용하거나 dropout을 적용하고 실험을 진행\n",
    "\n",
    "- **결과적으로 봤을 때 shortcut의 정보를 훼손시키지 않는 것이 제일 좋다고 함.**\n",
    "- **같은말로 shortcut path에 어떤 곱 연산을 시도하면 최적화하는데 방해가 된다는 것 – 그냥 더하기만 하게 냅두기 **\n",
    "![identify_mapping_shortcut](img/identify_mapping_shortcut.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 다양한 Identify Mapping 실험 - activation\n",
    "- Shortcut path를 손상시키지 않는 것이 가장 좋다는 것을 알았고, **활성화함수에 대한 실험을 진행**\n",
    "\n",
    "![identify_mapping_activation](img/identify_mapping_activation.png)\n",
    "- (a) 원래 것은 BN 까지 진행 후 Addition 후에 ReLU 를 입힘.\n",
    "- (b) BN 나중에\n",
    "- (c) BN, Relu 다 하고 addition\n",
    "- (d) Relu 를 먼저\n",
    "- (e) BN, Relu 먼저\n",
    "\n",
    "![identify_mapping_activation_result](img/identify_mapping_activation_result.jpg)\n",
    "- **활성화함수에서는 pre-activation을 적용한 모델의 경우가 더 낮았음.**\n",
    "- 단 기본 모델보다 training set에 대한 정확도는 낮지만, test에 대한 정확도는 더 높았고, 이러한 결과를 해석 하길\n",
    "    - BN이 모델의 regularization 역할을 수행했다. \n",
    "    - pre-activation을 적용한 경우 최적화가 훨씬 쉬워진 것 이라고 주장\n",
    "    \n",
    "![resnet_preactivation](img/resnet_preactivation.PNG)\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 후속논문 결론\n",
    "- **Shorcut path의 정보는 가능한 손상시키지 않는 것이 역전파, 정보의 전달 측면에서 유리하다**\n",
    "- **Residual path에서는 shortcut과 합쳐 주기 전에 activation을 취해주는 것이 조금 더 유리하다.**\n",
    "\n",
    "- **따라서 ResNet을 조금 더 개량한 오른쪽 모형을 제안**\n",
    "\n",
    "![identify_mapping_result](img/identify_mapping_result.jpg)\n",
    "![resnet_followup_residual](img/resnet_followup_residual.PNG)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
