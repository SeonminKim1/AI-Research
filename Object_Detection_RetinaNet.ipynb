{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RetinaNet \n",
    "- Paper : (2017) Focal Loss for Dense Object Detection\n",
    "- **기존의 One-stage Dense Detection 알고리즘 분류기에서의 Loss Function으로 사용되던 CE(cross entropy) ->  간단한 변형이 된 Focal Loss를 제안**\n",
    "- RetinaNet은 one-stage detector로 판단속도가 빠르고, two-stage detector보다 정확도가 높음\n",
    "\n",
    "![retinanet_graph](img/retinanet_graph.png)\n",
    "\n",
    "## RetinaNet 배경\n",
    "- Two-stage 기반 : 높은 정확도를 지님 / Classifier가 Sparse한 후보영역을 생성\n",
    "- One-stage 기반 : 처리속도 빠름 / 정확도가 낮은편 / dense detector / **전경과 배경이 극단적으로 불균형(imbalance)가 낮은 정확도의 원인**\n",
    "- **전경과 배경과의 class imbalance 문제를 해결하기 위해, 잘 분류된 샘플에 대해 가중치를 낮추는 방식으로 변형** 방법 제안\n",
    "- 즉 Focal Loss는 적은 수의 hard examples이 주어진 상태에서 많은 easy negatives sample을 지배적으로 학습해버리는 상황을 예방하는데 중점 **'Easy Negative Sample이 모델을 압도해버리는 문제'**\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## One stage vs Two stage\n",
    "- One-stage detector\n",
    "    - **Region proposal 없이, CNN Feature map에서 localization을 진행**\n",
    "    - 이를 위해 Anchor Box(미리 정의해서 계산 효율성 증가), Scale, aspect ratio(anchor box 가로세로비율) 등을 적용\n",
    "    - **Feature Map의 grid마다 anchor box를 적용하기에, one-stage보다 candidate object locations의 갯수가 훨씬 많음 (Object 갯수 < Foreground 갯수)**\n",
    "   \n",
    "- Two stage detector\n",
    "    - First stage에서 object가 존재할 확률이 높은 region proposal을 생성하기에 class 불균형 문제가 덜 민감\n",
    "    \n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Focal Loss\n",
    "\n",
    "### Balanced Cross Entropy\n",
    "- class의 불균형을 해결하기 위해 Cross Entropy에 weighting factor α를 적용\n",
    "- Positive/Negative example에 대한 차별성만 표현 가능\n",
    "\n",
    "![retinanet_balanced_crossentropy](img/retinanet_balanced_crossentropy.png)\n",
    "\n",
    "\n",
    "### Focal Loss Definition\n",
    "- 기존의 α-balanced cross entropy loss에 + $$ (1-p_t)^γ$$ 곱을 추가한 것\n",
    "- easy example에 대한 가중치를 줄인다\n",
    "    - easy example에 대한 loss를 줄임\n",
    "- hard example의 학습에 초점을 맞춘다.\n",
    "    - hard example에 대한 loss를 키움\n",
    "\n",
    "![retinanet_focalloss2](img/retinanet_focalloss2.png)\n",
    "\n",
    "- (1) 어떤 샘플이 분류가 잘못되고,$$ p_t $$ 가 작을 때, 조정 factor는 1에 가까운 값을 가지며, losssms 거의 영향이 없다. \n",
    "- (2) $$p_t$$ -> 1일때, 조정 factor가 0에 가까워 지고 잘 분류된 샘플에 대한 loss는 가중치가 작아짐\n",
    "- (3) focusing 파라미터인 γ는 easy 샘플들에 대한 가중치를 낮추는 정도를 somooth하게 조정\n",
    "- (4) γ=0일 때에는 FL은 CE와 같으며, γ가 증가하면 modulating factor의 효과가 증가하는 것과 같음\n",
    "- **즉 조정 Factor는 easy 샘플들의 loss에 대한 영향을 줄이고, 샘플이 낮은 loss를 받는 구간을 확장하여, Object 검출에 집중할 수 있게 도움**\n",
    "\n",
    "![retinanet_focalloss](img/retinanet_focalloss.png)\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RetinaNet Structure\n",
    "- RetinaNet은 **backbone 네트워크와 두개의 task-specific subnetworks로 구성**\n",
    "    - Backbone : Feature Extracting 수행\n",
    "        - ResNet\n",
    "        - FPN : Multi-scale conv feature pyramid -> Anchor box 생성\n",
    "    - subnetworks 1) Class subnet\n",
    "        - 각 anchor box의 **object가 존재할 확률 예측**\n",
    "        - 3*3 conv layers, ReLU activations\n",
    "    - subnetworks 2) Box subnet\n",
    "        - anchor box와 GT box를 비교 및 **bounding box를 추정** \n",
    "        - 앵커 박스의 offset 4개(box_x_center, box_y_center, box_width, box_height)를 GT박스와 유사하게 regression 함\n",
    "        - class-agnostic bounding box regressor를 사용 \n",
    "        - class subnet과 box subnet은 구조는 같지만 개별적인 파라미터를 사용 (파라미터 공유 x)\n",
    "![retinanet_structure](img/retinanet_structure.png)\n",
    "\n",
    "## RetinaNet result - 성능비교\n",
    "\n",
    "![retinanet_result](img/retinanet_result.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Focal loss 요약\n",
    "- 기존 One-stage의 정확도가 낮은 문제를 각 grid마다 Anchor Box들을 만드는데, Object보다 배경이 많이 항상 포함되는 문제 때문이라고 봄. (Two stage는 Region Proposal을 통해 Object가 있을만한 영역을 선 제안하기 떄문에 One-stage보다 덜함)\n",
    "- 이를 개선하기 위해 CE에서 Focal Loss라는 학습되기 쉬운 easy샘플들(배경)들이 주는 loss영향을 줄이고, 어려운 Hard샘플들, 보다 정밀한 Object 검출에 집중할 수 있게 도움.\n",
    "- RetinaNet : ResNet + FPN + Class Subnet(object 존재여부) + Box Subnet(box 좌표들 위치 조정)\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 참고문헌\n",
    "- https://uk-kim.github.io/2018/12/07/Focal-loss-for-dense-object-detection.html\n",
    "- https://ropiens.tistory.com/83"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
