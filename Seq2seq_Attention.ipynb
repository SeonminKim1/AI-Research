{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ffd3fd0",
   "metadata": {},
   "source": [
    "## Attention\n",
    "- (2017) Attention - Attention Is All You Need\n",
    "\n",
    "## 요약\n",
    "- convolution이나 recurrenct layer 없이 순수 attention만을 이용하여 feature extract. (Transformer 구조 제안)\n",
    "- self-attention을 multi head로 활용하여, input representation을 반복적으로 정제\n",
    "- Encoder-Decoder 네트워크 구조를 하고있으며, \n",
    "    - Encoder: representation learner\n",
    "    - Decoder: Auto-regressive Generator\n",
    "    \n",
    "### 1. seq2seq의 문제점\n",
    "- 기존 seq2seq에 쓰이던 **RNN 계열의 문제점은 Encoder의 출력이 고정 길이의 벡터로 데이터의 손실 및 공간 낭비의 문제가 발생한다는 점**\n",
    "    - 문장이 길어도 고정 길이 벡터로 출력됨\n",
    "- 본질적으로 hidden state $h_t$를 $h_(t-1)로 step의 input으로 생성하는 것은 $sequential한 특성은 훈련과정에서의 병렬화를 배제 됨\n",
    "    - 이는 더 긴 길이의 시퀀스를 처리할수록 critical 한 문제가 됨.\n",
    "\n",
    "![attention_seq2seq_problem](img/attention_seq2seq_problem.png)\n",
    "\n",
    "### 2. Encoder 개선\n",
    "- 이전엔 마지막 은닉 벡터(h)만을 Decoder에 전달 => 모든 계층상태(시각)에서의 은닉 벡터(h) 결과 벡터 전달 (return_sequences = True)\n",
    "- 모든 시각의 은닉 벡터를 꺼냄으로서, 입력 문장 길이에 비례한 정보 인코딩 가능\n",
    "![attention_encoder_problem](img/attention_encoder_problem.png)\n",
    "\n",
    "\n",
    "### 3. Decoder 개선 - Attention\n",
    "- 넘겨진 은닉 벡터 전체를 잘 활용하기 위한 개선\n",
    "- 한번에 모든 층의 결과를 받음으로서, 입력과 출력의 여러 단어 중 어떤 단어끼리 서로 관련되어 있는가에 대한 대응 관계를 학습이 가능하게 됨.\n",
    "- Attention은 필요한 정보에만 주목하여 그 정보로부터 시계열 변환을 수행\n",
    "- Attention 계층 설명\n",
    "    - 입력 데이터 : Encoder로부터 받는 hs와, 시각별 LSTM 계층의 은닉상태 (h)\n",
    "    \n",
    "![attention_decoder_upgrade](img/attention_decoder_upgrade.png)\n",
    "\n",
    "### 4. self-attention\n",
    "- 한 시퀀스의 representation을 계산하기 위해, 해당 단일 시퀀스 내에서 서로 다른 위치에 있는 요소들을 관련시키는 메커니즘"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
